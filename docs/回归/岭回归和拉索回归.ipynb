{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b980d28d-7f61-4a9d-9c2f-7d7110e3ef6c",
   "metadata": {},
   "source": [
    "# 岭回归和拉索回归\n",
    "\n",
    "## 最小二乘法的问题\n",
    "\n",
    "\n",
    "## 引入正则化\n",
    "\n",
    "\n",
    "岭回归（Ridge Regression）是一种用于解决线性回归问题的正则化方法。在普通最小二乘法（Ordinary Least Squares，简称OLS）中，我们试图通过最小化预测值与实际观测值之间的平方差来拟合一个线性模型。然而，在某些情况下，由于数据噪声或特征之间的多重共线性，OLS可能会产生不稳定的估计结果。\n",
    "\n",
    "岭回归通过引入一个正则化项，即岭惩罚项，来解决这个问题。岭回归的目标函数由两部分组成：一个是最小化拟合残差的平方和，另一个是控制参数向量的大小的惩罚项。\n",
    "\n",
    "具体地，岭回归的目标函数可表示为：\n",
    "\n",
    "$$\\underset{w}{min,} || Xw - y||_2^2 + \\alpha ||w||_2^2$$\n",
    "\n",
    "其中，$X$ 是输入数据矩阵，$y$ 是对应的观测值向量，$w$ 是待估计的参数向量，$\\alpha$ 是一个控制正则化强度的超参数。\n",
    "\n",
    "通过引入 $\\alpha$ 的值，岭回归可以对参数向量进行限制，使其不会过大，从而减小过拟合的风险。较大的 $\\alpha$ 值会增加正则化的程度，从而更加平衡模型的复杂度和拟合残差之间的权衡。\n",
    "\n",
    "岭回归使用了Tikhonov正则化的思想，通过在目标函数中添加正则化项来控制模型的复杂度。它是一种经典的线性回归扩展方法，在许多实际应用中都可以有效地提高模型的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e64426-803a-45d1-b94f-3e05d077e06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
